# papri_project_root-/docker-compose.yml
version: '3.9'

services:
  # Django Backend Application
  backend:
    build:
      context: ./backend # Path to the directory containing the Dockerfile
      dockerfile: Dockerfile
    container_name: papri_backend
    # The command to run when the container starts for the web service.
    # Using Gunicorn for production-like WSGI server.
    # For development, you might use: ["python", "manage.py", "runserver", "0.0.0.0:8000"]
    command: >
      sh -c "python manage.py wait_for_db &&
             python manage.py migrate &&
             python manage.py collectstatic --noinput &&
             gunicorn papri_project.wsgi:application --bind 0.0.0.0:8000 --workers 3 --log-level info"
    volumes:
      - ./backend:/app # Mounts the backend code directory for live reloading in dev
      - static_volume:/app/staticfiles_collected # Persist static files
      - media_volume:/app/mediafiles_storage   # Persist media files
    ports:
      - "8000:8000" # Maps port 8000 on the host to port 8000 in the container
    env_file:
      - ./backend/.env # Load environment variables from .env file in backend directory
    depends_on:
      db:
        condition: service_healthy # Wait for DB to be ready
      redis:
        condition: service_started # Wait for Redis to start
    restart: unless-stopped
    networks:
      - papri_network

  # MySQL Database
  db:
    image: mysql:8.0
    container_name: papri_db
    volumes:
      - mysql_data:/var/lib/mysql # Persist database data
      # You can mount custom MySQL config if needed:
      # - ./config/mysql/my.cnf:/etc/mysql/my.cnf
    environment:
      MYSQL_DATABASE: ${DB_NAME:-papri_db}
      MYSQL_USER: ${DB_USER:-papri_user}
      MYSQL_PASSWORD: ${DB_PASSWORD:-papri_password}
      MYSQL_ROOT_PASSWORD: ${DB_ROOT_PASSWORD:-supersecretpassword} # Change this for production
    ports:
      - "3307:3306" # Expose MySQL on host port 3307 to avoid conflict if local MySQL runs on 3306
    healthcheck:
      test: ["CMD", "mysqladmin" ,"ping", "-h", "localhost", "-u$$MYSQL_USER", "-p$$MYSQL_PASSWORD"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - papri_network

  # Redis (for Celery Broker & Cache)
  redis:
    image: redis:7-alpine
    container_name: papri_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data # Persist Redis data
    restart: unless-stopped
    networks:
      - papri_network

  # Celery Worker - Default Queue (for quick tasks)
  celery_worker_default:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: papri_celery_default
    # Command to start a Celery worker listening to the 'default' queue.
    # Adjust concurrency (-c) as needed.
    command: >
      sh -c "python manage.py wait_for_db &&
             celery -A papri_project worker -l info -Q default -c 2"
    volumes:
      - ./backend:/app
    env_file:
      - ./backend/.env
    depends_on:
      - backend # Ensure backend image is built, and settings/code are available
      - db
      - redis
    restart: unless-stopped
    networks:
      - papri_network

  # Celery Worker - AI Processing Queue
  celery_worker_ai:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: papri_celery_ai
    command: >
      sh -c "python manage.py wait_for_db &&
             celery -A papri_project worker -l info -Q ai_processing -c 1" # Lower concurrency for heavy tasks
    volumes:
      - ./backend:/app
      # If AI models are large and not part of the image, mount them here:
      # - /path/to/your/ai_models_on_host:/app/ai_models 
    env_file:
      - ./backend/.env
    depends_on:
      - backend
      - db
      - redis
    # Potentially add resource limits here if these workers are very resource-intensive
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '2'
    #       memory: '4G'
    restart: unless-stopped
    networks:
      - papri_network

  # Celery Worker - Video Editing Queue
  celery_worker_video:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: papri_celery_video
    command: >
      sh -c "python manage.py wait_for_db &&
             celery -A papri_project worker -l info -Q video_editing -c 1"
    volumes:
      - ./backend:/app
      - media_volume:/app/mediafiles_storage # Ensure worker can access/write media
    env_file:
      - ./backend/.env
    depends_on:
      - backend
      - db
      - redis
    restart: unless-stopped
    networks:
      - papri_network

  # Celery Beat (Scheduler for periodic tasks)
  celery_beat:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: papri_celery_beat
    command: >
      sh -c "python manage.py wait_for_db &&
             celery -A papri_project beat -l info --scheduler django_celery_beat.schedulers:DatabaseScheduler"
    volumes:
      - ./backend:/app
    env_file:
      - ./backend/.env
    depends_on:
      - backend
      - db
      - redis
    restart: unless-stopped
    networks:
      - papri_network

  # Qdrant Vector Database (Optional, if running locally via Docker)
  # If using a cloud Qdrant, you don't need this service.
  # qdrant:
  #   image: qdrant/qdrant:latest # Check for a stable version tag
  #   container_name: papri_qdrant
  #   ports:
  #     - "6333:6333" # REST API
  #     - "6334:6334" # gRPC
  #   volumes:
  #     - qdrant_storage:/qdrant/storage # Persist Qdrant data
  #   # You might need to provide a Qdrant config file if not using defaults
  #   # environment:
  #   #   QDRANT__SERVICE__API_KEY: ${QDRANT_API_KEY:-} # If you use API keys with local Qdrant
  #   restart: unless-stopped
  #   networks:
  #     - papri_network

# Docker Networks
networks:
  papri_network:
    driver: bridge

# Docker Volumes (to persist data)
volumes:
  mysql_data:
  redis_data:
  static_volume:
  media_volume:
  # qdrant_storage: # If using local Qdrant service
